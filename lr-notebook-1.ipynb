{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-07T13:15:52.055939Z",
     "iopub.status.busy": "2023-03-07T13:15:52.055511Z",
     "iopub.status.idle": "2023-03-07T13:15:52.108756Z",
     "shell.execute_reply": "2023-03-07T13:15:52.107588Z",
     "shell.execute_reply.started": "2023-03-07T13:15:52.055900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/README.md\n",
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/model/config.json\n",
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/model/merges.txt\n",
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/model/sentence_bert_config.json\n",
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/model/pytorch_model.bin\n",
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/model/config_sentence_transformers.json\n",
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/model/modules.json\n",
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/tokenizer/tokenizer.json\n",
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/tokenizer/vocab.json\n",
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/tokenizer/tokenizer_config.json\n",
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/tokenizer/special_tokens_map.json\n",
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/config/config.json\n",
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/config/merges.txt\n",
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/config/sentence_bert_config.json\n",
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/config/config_sentence_transformers.json\n",
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/config/modules.json\n",
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/1_Pooling/config.json\n",
      "/kaggle/input/paraphrase-distilroberta-base-v1/paraphrase-distilroberta-base-v1-exp2/.ipynb_checkpoints/README-checkpoint.md\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/README.md\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/model/config.json\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/model/merges.txt\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/model/sentence_bert_config.json\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/model/pytorch_model.bin\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/model/config_sentence_transformers.json\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/model/modules.json\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/tokenizer/tokenizer.json\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/tokenizer/vocab.json\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/tokenizer/tokenizer_config.json\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/tokenizer/special_tokens_map.json\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/config/config.json\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/config/merges.txt\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/config/sentence_bert_config.json\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/config/config_sentence_transformers.json\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/config/modules.json\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/1_Pooling/config.json\n",
      "/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1/.ipynb_checkpoints/README-checkpoint.md\n",
      "/kaggle/input/learning-equality-curriculum-recommendations/sample_submission.csv\n",
      "/kaggle/input/learning-equality-curriculum-recommendations/topics.csv\n",
      "/kaggle/input/learning-equality-curriculum-recommendations/correlations.csv\n",
      "/kaggle/input/learning-equality-curriculum-recommendations/content.csv\n",
      "/kaggle/input/xlm-roberta/tensorflow2/multi-cased-l-12-h-768-a-12/1/saved_model.pb\n",
      "/kaggle/input/xlm-roberta/tensorflow2/multi-cased-l-12-h-768-a-12/1/keras_metadata.pb\n",
      "/kaggle/input/xlm-roberta/tensorflow2/multi-cased-l-12-h-768-a-12/1/variables/variables.index\n",
      "/kaggle/input/xlm-roberta/tensorflow2/multi-cased-l-12-h-768-a-12/1/variables/variables.data-00000-of-00001\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/xlm-roberta-base_fold0_42.pth\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-mpnet-base-v2/model/config.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-mpnet-base-v2/model/pytorch_model.bin\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-mpnet-base-v2/tokenizer/tokenizer.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-mpnet-base-v2/tokenizer/tokenizer_config.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-mpnet-base-v2/tokenizer/special_tokens_map.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-mpnet-base-v2/tokenizer/vocab.txt\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-mpnet-base-v2/config/config.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-MiniLM-L6-v2/model/config.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-MiniLM-L6-v2/model/pytorch_model.bin\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-MiniLM-L6-v2/model/.ipynb_checkpoints/config-checkpoint.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-MiniLM-L6-v2/tokenizer/tokenizer.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-MiniLM-L6-v2/tokenizer/tokenizer_config.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-MiniLM-L6-v2/tokenizer/special_tokens_map.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-MiniLM-L6-v2/tokenizer/vocab.txt\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-MiniLM-L6-v2/tokenizer/.ipynb_checkpoints/tokenizer_config-checkpoint.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-MiniLM-L6-v2/tokenizer/.ipynb_checkpoints/special_tokens_map-checkpoint.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-MiniLM-L6-v2/tokenizer/.ipynb_checkpoints/tokenizer-checkpoint.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-MiniLM-L6-v2/config/config.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-MiniLM-L6-v2/config/.ipynb_checkpoints/config-checkpoint.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/xlm-roberta-base/model/config.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/xlm-roberta-base/model/pytorch_model.bin\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/xlm-roberta-base/tokenizer/tokenizer.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/xlm-roberta-base/tokenizer/tokenizer_config.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/xlm-roberta-base/tokenizer/special_tokens_map.json\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/xlm-roberta-base/tokenizer/sentencepiece.bpe.model\n",
      "/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/xlm-roberta-base/config/config.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and setting up configuration classes for pretrained model\n",
    "\n",
    "This code defines three configurations for three different models: <code>uns_model</code> and <code>sup_model</code> for each configuration correspond to the paths of pre-trained models for unsupervised and supervised learning respectively. <code>uns_tokenizer</code> and <code>sup_tokenizer</code> for each configuration correspond to the tokenizers for unsupervised and supervised learning respectively. The <code>pooling</code> attribute for each configuration indicates the way of pooling embeddings. <code>gradient_checkpointing</code> is a flag to determine whether to use gradient checkpointing for more efficient memory usage. <code>add_with_best_prob</code> is a flag indicating whether to add the embedding with the best probability during the training phase.\n",
    "\n",
    "Finally, the code creates a list <code>CFG_list</code> that contains all the defined configurations, which will be used in the next steps of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:16:54.811036Z",
     "iopub.status.busy": "2023-03-07T13:16:54.810571Z",
     "iopub.status.idle": "2023-03-07T13:17:07.537652Z",
     "shell.execute_reply": "2023-03-07T13:17:07.536456Z",
     "shell.execute_reply.started": "2023-03-07T13:16:54.810995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "import cupy as cp\n",
    "from cuml.metrics import pairwise_distances\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class CFG1:\n",
    "    uns_model = \"/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-mpnet-base-v2\"\n",
    "    sup_model = \"/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/xlm-roberta-base\"\n",
    "    uns_tokenizer = AutoTokenizer.from_pretrained(uns_model + '/tokenizer')\n",
    "    sup_tokenizer = AutoTokenizer.from_pretrained(sup_model + '/tokenizer')\n",
    "    pooling = \"mean\"\n",
    "    gradient_checkpointing = False\n",
    "    add_with_best_prob = True\n",
    "    \n",
    "class CFG2:\n",
    "    uns_model = \"/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/sentence-transformers-all-MiniLM-L6-v2\"\n",
    "    sup_model = \"/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/xlm-roberta-base\"\n",
    "    uns_tokenizer = AutoTokenizer.from_pretrained(uns_model + '/tokenizer')\n",
    "    sup_tokenizer = AutoTokenizer.from_pretrained(sup_model + '/tokenizer')\n",
    "    pooling = \"mean\"\n",
    "    gradient_checkpointing = False\n",
    "    add_with_best_prob = True\n",
    "    \n",
    "class CFG3:\n",
    "    uns_model = \"/kaggle/input/stsb-roberta-base-v2/stsb-roberta-base-v2-exp1\"\n",
    "    sup_model = \"/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/xlm-roberta-base\"\n",
    "    uns_tokenizer = AutoTokenizer.from_pretrained(uns_model + '/tokenizer')\n",
    "    sup_tokenizer = AutoTokenizer.from_pretrained(sup_model + '/tokenizer')\n",
    "    pooling = \"mean\"\n",
    "    gradient_checkpointing = False\n",
    "    add_with_best_prob = True \n",
    "    \n",
    "CFG_list = [CFG1, CFG2, CFG3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function for reading the data into RAM with basic preproccessing\n",
    "This is a Python function named <code>read_data(cfg)</code> that takes a configuration object <code>cfg</code> as an input parameter.\n",
    "\n",
    "The function reads in three CSV files: <code>topics.csv</code>, <code>content.csv</code>, and <code>sample_submission.csv</code>, which contain data related to the Learning Curriculum Recommendation project.\n",
    "\n",
    "After reading in the CSV files, the function merges the <code>topics</code> dataframe with the <code>sample_submission</code> dataframe, which allows the function to only infer test topics.\n",
    "\n",
    "Next, the function fills in missing values in the <code>title</code> column of both the <code>topics</code> and <code>content</code> dataframes.\n",
    "\n",
    "Then, the function adds a new column called <code>length</code> to both dataframes, which contains the length of the <code>title</code> column for each row. The dataframes are then sorted in ascending order by the <code>length</code> column to make inference faster.\n",
    "\n",
    "The function drops several columns from both dataframes that are not needed for the project, and then resets the index of both dataframes.\n",
    "\n",
    "Finally, the function prints the shapes of both dataframes to verify that they have been read in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:17:20.413655Z",
     "iopub.status.busy": "2023-03-07T13:17:20.413294Z",
     "iopub.status.idle": "2023-03-07T13:17:20.423405Z",
     "shell.execute_reply": "2023-03-07T13:17:20.422229Z",
     "shell.execute_reply.started": "2023-03-07T13:17:20.413622Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(cfg):\n",
    "    topics = pd.read_csv('/kaggle/input/learning-equality-curriculum-recommendations/topics.csv')\n",
    "    content = pd.read_csv('/kaggle/input/learning-equality-curriculum-recommendations/content.csv')\n",
    "    sample_submission = pd.read_csv('/kaggle/input/learning-equality-curriculum-recommendations/sample_submission.csv')\n",
    "    # Merge topics with sample submission to only infer test topics\n",
    "    topics = topics.merge(sample_submission, how = 'inner', left_on = 'id', right_on = 'topic_id')\n",
    "    # Fillna titles\n",
    "    topics['title'].fillna(\"\", inplace = True)\n",
    "    content['title'].fillna(\"\", inplace = True)\n",
    "    # Sort by title length to make inference faster\n",
    "    topics['length'] = topics['title'].apply(lambda x: len(x))\n",
    "    content['length'] = content['title'].apply(lambda x: len(x))\n",
    "    topics.sort_values('length', inplace = True)\n",
    "    content.sort_values('length', inplace = True)\n",
    "    # Drop cols\n",
    "    topics.drop(['description', 'channel', 'category', 'level', 'parent', 'has_content', 'length', 'topic_id', 'content_ids'], axis = 1, inplace = True)\n",
    "    content.drop(['description', 'kind', 'text', 'copyright_holder', 'license', 'length'], axis = 1, inplace = True)\n",
    "    # Reset index\n",
    "    topics.reset_index(drop = True, inplace = True)\n",
    "    content.reset_index(drop = True, inplace = True)\n",
    "    print(' ')\n",
    "    print('-' * 50)\n",
    "    print(f\"topics.shape: {topics.shape}\")\n",
    "    print(f\"content.shape: {content.shape}\")\n",
    "    return topics, content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tensors for input to unsupervised model that is encoded tokenizers\n",
    "\n",
    "This code defines a function <code>prepare_uns_input</code> that takes two arguments: <code>text</code> and <code>cfg</code>. The function uses the <code>uns_tokenizer</code> attribute of the configuration object <code>cfg</code> to tokenize the input <code>text</code>. The resulting tokenized input is encoded using <code>encode_plus</code> method of the tokenizer. The <code>return_tensors</code> argument is set to <code>None</code>, meaning that the function will not return the encoded input as tensors. The <code>add_special_tokens</code> argument is set to <code>True</code>, meaning that the special tokens of the tokenizer will be added to the encoded input.\n",
    "\n",
    "The resulting encoded input is stored in a dictionary called <code>inputs</code>, which is then looped over to convert all values to PyTorch tensors of data type <code>long</code>. Finally, the function returns the encoded and tensorized input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:17:25.540792Z",
     "iopub.status.busy": "2023-03-07T13:17:25.540390Z",
     "iopub.status.idle": "2023-03-07T13:17:25.546963Z",
     "shell.execute_reply": "2023-03-07T13:17:25.545701Z",
     "shell.execute_reply.started": "2023-03-07T13:17:25.540752Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_uns_input(text, cfg):\n",
    "    inputs = cfg.uns_tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors = None, \n",
    "        add_special_tokens = True, \n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype = torch.long)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Defining Pooling classes** viz, \n",
    "\n",
    "This is a module defining several pooling layers used in natural language processing tasks, where the outputs of a pre-trained language model (such as <code>BERT</code>) are passed through these pooling layers to obtain a fixed-size representation of the input text. The fixed-size representation can then be used for downstream tasks, such as classification or regression.\n",
    "\n",
    "The module defines the following pooling layers:\n",
    "\n",
    "- <code>MeanPooling</code>: computes the mean of the hidden state vectors over the input sequence, weighted by the attention mask to exclude padding tokens.\n",
    "- <code>AttentionPooling</code>: computes a weighted sum of the hidden state vectors over the input sequence, where the weights are computed using a multi-layer perceptron (<code>MLP</code>) and then normalized with a softmax function, again weighted by the attention mask to exclude padding tokens.\n",
    "- <code>MaxPooling</code>: computes the element-wise maximum of the hidden state vectors over the input sequence, where the hidden state vectors are set to a very negative value (-10^4) for padding tokens so they are not selected as the maximum.\n",
    "- <code>MinPooling</code>: computes the element-wise minimum of the hidden state vectors over the input sequence, where the hidden state vectors are set to a very small positive value (10^-4) for padding tokens so they are not selected as the minimum.\n",
    "- <code>WeightedLayerPooling</code>: computes a weighted average of the hidden state vectors over all the layers of the pre-trained language model, where the weights can be learned or manually set.\n",
    "- <code>ConcatPooling</code>: concatenates the hidden state vectors from the last <code>n_layers</code> of the pre-trained language model, where <code>n_layers</code> is a hyperparameter specified in the <code>pooling_config</code>, and returns the concatenated vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:17:29.456675Z",
     "iopub.status.busy": "2023-03-07T13:17:29.456235Z",
     "iopub.status.idle": "2023-03-07T13:17:29.476675Z",
     "shell.execute_reply": "2023-03-07T13:17:29.475392Z",
     "shell.execute_reply.started": "2023-03-07T13:17:29.456638Z"
    }
   },
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        w = self.attention(last_hidden_state).float()\n",
    "        w[attention_mask==0]=float('-inf')\n",
    "        w = torch.softmax(w,1)\n",
    "        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n",
    "        return attention_embeddings\n",
    "\n",
    "class MaxPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxPooling, self).__init__()       \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        embeddings = last_hidden_state.clone()\n",
    "        embeddings[input_mask_expanded == 0] = -1e4\n",
    "        max_embeddings, _ = torch.max(embeddings, dim = 1)\n",
    "        return max_embeddings\n",
    "       \n",
    "class MinPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MinPooling, self).__init__()     \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        embeddings = last_hidden_state.clone()\n",
    "        embeddings[input_mask_expanded == 0] = 1e-4\n",
    "        min_embeddings, _ = torch.min(embeddings, dim = 1)\n",
    "        return min_embeddings\n",
    "    \n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n",
    "            )\n",
    "    def forward(self, features):\n",
    "        ft_all_layers = features['all_layer_embeddings']\n",
    "\n",
    "        all_layer_embedding = torch.stack(ft_all_layers)\n",
    "        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n",
    "\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "\n",
    "        features.update({'token_embeddings': weighted_average})\n",
    "        return features\n",
    "    \n",
    "class ConcatPooling(nn.Module):\n",
    "    def __init__(self, backbone_config, pooling_config):\n",
    "        super(ConcatPooling, self, ).__init__()\n",
    "        self.n_layers = pooling_config.n_layers\n",
    "        self.output_dim = backbone_config.hidden_size*pooling_config.n_layers\n",
    "\n",
    "    def forward(self, inputs, backbone_outputs):\n",
    "        all_hidden_states = get_all_hidden_states(backbone_outputs)\n",
    "\n",
    "        concatenate_pooling = torch.cat([all_hidden_states[-(i + 1)] for i in range(self.n_layers)], -1)\n",
    "        concatenate_pooling = concatenate_pooling[:, 0]\n",
    "        return concatenate_pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Dataset\n",
    "\n",
    "This is a Python class definition for a custom dataset called <code>uns_dataset</code> which inherits from the PyTorch <code>Dataset</code> class.\n",
    "\n",
    "The <code>__init__</code> method initializes the dataset by taking in two arguments: <code>df</code>, which is a pandas dataframe containing a column named <code>title</code> with the input text data, and <code>cfg</code>, which is a configuration object that contains parameters related to the text preprocessing and encoding.\n",
    "\n",
    "The <code>__len__</code> method is required by the <code>Dataset</code> class and returns the length of the dataset. In this case, it returns the length of the <code>texts</code> attribute, which is a numpy array of the text data.\n",
    "\n",
    "The <code>__getitem__</code> method is also required by the <code>Dataset</code> class and defines how to retrieve an individual data sample from the dataset. In this case, it takes an index <code>item</code> as an argument, prepares the input data using a <code>prepare_uns_input</code> function with the text at that index and the <code>cfg</code> configuration object, and returns the prepared input data.\n",
    "\n",
    "Overall, this class allows the text data to be loaded and prepared on the fly as needed during training or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:17:36.068661Z",
     "iopub.status.busy": "2023-03-07T13:17:36.068197Z",
     "iopub.status.idle": "2023-03-07T13:17:36.075125Z",
     "shell.execute_reply": "2023-03-07T13:17:36.073964Z",
     "shell.execute_reply.started": "2023-03-07T13:17:36.068625Z"
    }
   },
   "outputs": [],
   "source": [
    "class uns_dataset(Dataset):\n",
    "    def __init__(self, df, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['title'].values\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_uns_input(self.texts[item], self.cfg)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised Model Architechture and preparing input for the same**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:17:39.620579Z",
     "iopub.status.busy": "2023-03-07T13:17:39.620205Z",
     "iopub.status.idle": "2023-03-07T13:17:39.628517Z",
     "shell.execute_reply": "2023-03-07T13:17:39.627223Z",
     "shell.execute_reply.started": "2023-03-07T13:17:39.620543Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_sup_input(text, cfg):\n",
    "    inputs = cfg.sup_tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors = None, \n",
    "        add_special_tokens = True, \n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype = torch.long)\n",
    "    return inputs\n",
    "\n",
    "class sup_dataset(Dataset):\n",
    "    def __init__(self, df, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_sup_input(self.texts[item], self.cfg)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised model architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:17:43.569997Z",
     "iopub.status.busy": "2023-03-07T13:17:43.569299Z",
     "iopub.status.idle": "2023-03-07T13:17:43.577973Z",
     "shell.execute_reply": "2023-03-07T13:17:43.576777Z",
     "shell.execute_reply.started": "2023-03-07T13:17:43.569954Z"
    }
   },
   "outputs": [],
   "source": [
    "class uns_model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.config = AutoConfig.from_pretrained(cfg.uns_model + '/config')\n",
    "        self.model = AutoModel.from_pretrained(cfg.uns_model + '/model', config = self.config)\n",
    "        self.pool = MeanPooling()\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        feature = self.pool(last_hidden_state, inputs['attention_mask'])\n",
    "        return feature\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:17:46.819758Z",
     "iopub.status.busy": "2023-03-07T13:17:46.819358Z",
     "iopub.status.idle": "2023-03-07T13:17:46.826705Z",
     "shell.execute_reply": "2023-03-07T13:17:46.825673Z",
     "shell.execute_reply.started": "2023-03-07T13:17:46.819695Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings(loader, model, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for step, inputs in enumerate(tqdm(loader)):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Producing Positive classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:17:50.130162Z",
     "iopub.status.busy": "2023-03-07T13:17:50.129776Z",
     "iopub.status.idle": "2023-03-07T13:17:50.137922Z",
     "shell.execute_reply": "2023-03-07T13:17:50.136684Z",
     "shell.execute_reply.started": "2023-03-07T13:17:50.130130Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pos_socre(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    int_true = np.array([len(x[0] & x[1]) / len(x[0]) for x in zip(y_true, y_pred)])\n",
    "    return round(np.mean(int_true), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building our inference set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:17:55.243339Z",
     "iopub.status.busy": "2023-03-07T13:17:55.242955Z",
     "iopub.status.idle": "2023-03-07T13:17:55.253653Z",
     "shell.execute_reply": "2023-03-07T13:17:55.252312Z",
     "shell.execute_reply.started": "2023-03-07T13:17:55.243302Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_inference_set(topics, content, cfg):\n",
    "    # Create lists for training\n",
    "    topics_ids = []\n",
    "    content_ids = []\n",
    "    topics_languages = []\n",
    "    content_languages = []\n",
    "    title1 = []\n",
    "    title2 = []\n",
    "    # Iterate over each topic\n",
    "    for k in tqdm(range(len(topics))):\n",
    "        row = topics.iloc[k]\n",
    "        topics_id = row['id']\n",
    "        topics_language = row['language']\n",
    "        topics_title = row['title']\n",
    "        predictions = row['predictions'].split(' ')\n",
    "        for pred in predictions:\n",
    "            content_title = content.loc[pred, 'title']\n",
    "            content_language = content.loc[pred, 'language']\n",
    "            topics_ids.append(topics_id)\n",
    "            content_ids.append(pred)\n",
    "            title1.append(topics_title)\n",
    "            title2.append(content_title)\n",
    "            topics_languages.append(topics_language)\n",
    "            content_languages.append(content_language)\n",
    "    # Build training dataset\n",
    "    test = pd.DataFrame(\n",
    "        {'topics_ids': topics_ids, \n",
    "         'content_ids': content_ids, \n",
    "         'title1': title1, \n",
    "         'title2': title2,\n",
    "         'topic_language': topics_languages, \n",
    "         'content_language': content_languages, \n",
    "        }\n",
    "    )\n",
    "    # Release memory\n",
    "    del topics_ids, content_ids, title1, title2, topics_languages, content_languages\n",
    "    gc.collect()\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Neighbours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:18:02.350411Z",
     "iopub.status.busy": "2023-03-07T13:18:02.348055Z",
     "iopub.status.idle": "2023-03-07T13:18:02.370960Z",
     "shell.execute_reply": "2023-03-07T13:18:02.370010Z",
     "shell.execute_reply.started": "2023-03-07T13:18:02.350349Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_neighbors(tmp_topics, tmp_content, cfg):\n",
    "    # Create topics dataset\n",
    "    topics_dataset = uns_dataset(tmp_topics, cfg)\n",
    "    # Create content dataset\n",
    "    content_dataset = uns_dataset(tmp_content, cfg)\n",
    "    # Create topics and content dataloaders\n",
    "    topics_loader = DataLoader(\n",
    "        topics_dataset, \n",
    "        batch_size = 32, \n",
    "        shuffle = False, \n",
    "        collate_fn = DataCollatorWithPadding(tokenizer = cfg.uns_tokenizer, padding = 'longest'),\n",
    "        num_workers = 4, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "    )\n",
    "    content_loader = DataLoader(\n",
    "        content_dataset, \n",
    "        batch_size = 32, \n",
    "        shuffle = False, \n",
    "        collate_fn = DataCollatorWithPadding(tokenizer = cfg.uns_tokenizer, padding = 'longest'),\n",
    "        num_workers = 4, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "        )\n",
    "    # Create unsupervised model to extract embeddings\n",
    "    model = uns_model(cfg)\n",
    "    model.to(device)\n",
    "    # Predict topics\n",
    "    topics_preds = get_embeddings(topics_loader, model, device)\n",
    "    content_preds = get_embeddings(content_loader, model, device)\n",
    "    # Transfer predictions to gpu\n",
    "    topics_preds_gpu = cp.array(topics_preds)\n",
    "    content_preds_gpu = cp.array(content_preds)\n",
    "    # Release memory\n",
    "    del topics_dataset, content_dataset, topics_loader, content_loader, topics_preds, content_preds\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # KNN model\n",
    "    print(' ')\n",
    "    print('Training KNN model...')\n",
    "    neighbors_model = NearestNeighbors(n_neighbors = 1000, metric = 'cosine')\n",
    "    neighbors_model.fit(content_preds_gpu)\n",
    "    indices = neighbors_model.kneighbors(topics_preds_gpu, return_distance = False)\n",
    "    predictions = []\n",
    "    for k in range(len(indices)):\n",
    "        pred = indices[k]\n",
    "        p = ' '.join([tmp_content.loc[ind, 'id'] for ind in pred.get()])\n",
    "        predictions.append(p)\n",
    "    tmp_topics['predictions'] = predictions\n",
    "    # Release memory\n",
    "    del topics_preds_gpu, content_preds_gpu, neighbors_model, predictions, indices, model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return tmp_topics, tmp_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processing Test DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:18:07.050846Z",
     "iopub.status.busy": "2023-03-07T13:18:07.050206Z",
     "iopub.status.idle": "2023-03-07T13:18:07.059582Z",
     "shell.execute_reply": "2023-03-07T13:18:07.057785Z",
     "shell.execute_reply.started": "2023-03-07T13:18:07.050804Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_test(tmp_test):\n",
    "    tmp_test['title1'].fillna(\"Title does not exist\", inplace = True)\n",
    "    tmp_test['title2'].fillna(\"Title does not exist\", inplace = True)\n",
    "    # Create feature column\n",
    "    tmp_test['text'] = tmp_test['title1'] + '[SEP]' + tmp_test['title2']\n",
    "    # Drop titles\n",
    "    tmp_test.drop(['title1', 'title2'], axis = 1, inplace = True)\n",
    "    # Sort so inference is faster\n",
    "    tmp_test['length'] = tmp_test['text'].apply(lambda x: len(x))\n",
    "    tmp_test.sort_values('length', inplace = True)\n",
    "    tmp_test.drop(['length'], axis = 1, inplace = True)\n",
    "    tmp_test.reset_index(drop = True, inplace = True)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return tmp_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Itself**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:18:11.252435Z",
     "iopub.status.busy": "2023-03-07T13:18:11.251870Z",
     "iopub.status.idle": "2023-03-07T13:18:11.272057Z",
     "shell.execute_reply": "2023-03-07T13:18:11.270855Z",
     "shell.execute_reply.started": "2023-03-07T13:18:11.252390Z"
    }
   },
   "outputs": [],
   "source": [
    "class custom_model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.config = AutoConfig.from_pretrained(cfg.sup_model + '/config', output_hidden_states = True)\n",
    "        self.config.hidden_dropout = 0.0\n",
    "        self.config.hidden_dropout_prob = 0.0\n",
    "        self.config.attention_dropout = 0.0\n",
    "        self.config.attention_probs_dropout_prob = 0.0\n",
    "        self.model = AutoModel.from_pretrained(cfg.sup_model + '/model', config = self.config)\n",
    "        #self.pool = MeanPooling()\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        if CFG.pooling == 'mean' or CFG.pooling == \"ConcatPool\":\n",
    "            self.pool = MeanPooling()\n",
    "        elif CFG.pooling == 'max':\n",
    "            self.pool = MaxPooling()\n",
    "        elif CFG.pooling == 'min':\n",
    "            self.pool = MinPooling()\n",
    "        elif CFG.pooling == 'attention':\n",
    "            self.pool = AttentionPooling(self.config.hidden_size)\n",
    "        elif CFG.pooling == \"WLP\":\n",
    "            self.pool = WeightedLayerPooling(self.config.num_hidden_layers, layer_start=6)\n",
    "        \n",
    "        if CFG.pooling == \"ConcatPool\":\n",
    "            self.fc = nn.Linear(self.config.hidden_size*4, 1)  \n",
    "        else:\n",
    "            self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        #self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        \n",
    "        if CFG.pooling == \"WLP\":\n",
    "            last_hidden_state = self.model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "            tmp = {\n",
    "                'all_layer_embeddings': last_hidden_state.hidden_states\n",
    "            }\n",
    "            feature = self.pool(tmp)['token_embeddings'][:, 0]\n",
    "            \n",
    "        elif CFG.pooling == \"ConcatPool\":\n",
    "            last_hidden_state = torch.stack(self.model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask']).hidden_states)\n",
    "\n",
    "            p1 = self.pool(last_hidden_state[-1], inputs['attention_mask'])\n",
    "            p2 = self.pool(last_hidden_state[-2], inputs['attention_mask'])\n",
    "            p3 = self.pool(last_hidden_state[-3], inputs['attention_mask'])\n",
    "            p4 = self.pool(last_hidden_state[-4], inputs['attention_mask'])\n",
    "\n",
    "            feature = torch.cat(\n",
    "                (p1, p2, p3, p4),-1\n",
    "            )\n",
    "               \n",
    "        else:\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            feature = self.pool(last_hidden_state, inputs['attention_mask'])\n",
    "        \n",
    "        #last_hidden_state = outputs.last_hidden_state\n",
    "        #feature = self.pool(last_hidden_state, inputs['attention_mask'])\n",
    "        return feature\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:18:17.377966Z",
     "iopub.status.busy": "2023-03-07T13:18:17.377442Z",
     "iopub.status.idle": "2023-03-07T13:18:17.405108Z",
     "shell.execute_reply": "2023-03-07T13:18:17.403877Z",
     "shell.execute_reply.started": "2023-03-07T13:18:17.377919Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total = len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.sigmoid().squeeze().to('cpu').numpy().reshape(-1))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "def inference(test, cfg, _idx):\n",
    "    # Create dataset and loader\n",
    "    test_dataset = sup_dataset(test, cfg)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size = 32, \n",
    "        shuffle = False, \n",
    "        collate_fn = DataCollatorWithPadding(tokenizer = cfg.sup_tokenizer, padding = 'longest'),\n",
    "        num_workers = 2, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "    )\n",
    "    # Get model\n",
    "    model = custom_model(cfg)\n",
    "    \n",
    "    # Load weights\n",
    "    state = torch.load(\"/kaggle/input/lecr-ensemble/LECR-ENSEMBLE/xlm-roberta-base_fold0_42.pth\", map_location = torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    prediction = inference_fn(test_loader, model, device)\n",
    "    \n",
    "    # Release memory\n",
    "    torch.cuda.empty_cache()\n",
    "    del test_dataset, test_loader, model, state\n",
    "    gc.collect()\n",
    "    \n",
    "    # Use threshold\n",
    "    test['probs'] = prediction\n",
    "    test['predictions'] = test['probs'].apply(lambda x: int(x > 0.0006))  \n",
    "    test = test.merge(test.groupby(\"topics_ids\", as_index=False)[\"probs\"].max(), on=\"topics_ids\", suffixes=[\"\", \"_max\"])\n",
    "    display(test.head())\n",
    "    \n",
    "    test1 = test[(test['predictions'] == 1) & (test['topic_language'] == test['content_language'])]\n",
    "    test1 = test1.groupby(['topics_ids'])['content_ids'].unique().reset_index()\n",
    "    test1['content_ids'] = test1['content_ids'].apply(lambda x: ' '.join(x))\n",
    "    test1.columns = ['topic_id', 'content_ids']\n",
    "    display(test1.head())\n",
    "    \n",
    "    test0 = pd.Series(test['topics_ids'].unique())\n",
    "    test0 = test0[~test0.isin(test1['topic_id'])]\n",
    "    test0 = pd.DataFrame({'topic_id': test0.values, 'content_ids': \"\"})\n",
    "    if cfg.add_with_best_prob:\n",
    "        test0 = test0[[\"topic_id\"]].merge(test[test['probs'] == test['probs_max']][[\"topics_ids\", \"content_ids\"]],\n",
    "                                          left_on=\"topic_id\", right_on=\"topics_ids\")[['topic_id', \"content_ids\"]]\n",
    "    display(test0.head())\n",
    "    test_r = pd.concat([test1, test0], axis = 0, ignore_index = True)\n",
    "    test_r.to_csv(f'submission_{_idx+1}.csv', index = False)\n",
    "    \n",
    "    return test_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:18:22.107268Z",
     "iopub.status.busy": "2023-03-07T13:18:22.106195Z",
     "iopub.status.idle": "2023-03-07T13:32:14.694902Z",
     "shell.execute_reply": "2023-03-07T13:32:14.693770Z",
     "shell.execute_reply.started": "2023-03-07T13:18:22.107214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "topics.shape: (5, 3)\n",
      "content.shape: (154047, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61158222b76a4134a0ec17abc92a8a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3858b13e49243f386100c2f1be08b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Training KNN model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1338e2cce7483faa4d6a62a7926f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449c8357b3ee4001a59ef145ec953898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics_ids</th>\n",
       "      <th>content_ids</th>\n",
       "      <th>topic_language</th>\n",
       "      <th>content_language</th>\n",
       "      <th>text</th>\n",
       "      <th>probs</th>\n",
       "      <th>predictions</th>\n",
       "      <th>probs_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_66111e868395</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Transcripts[SEP]DNA</td>\n",
       "      <td>4.851724e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_bd5e71a65b93</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Transcripts[SEP]DNA</td>\n",
       "      <td>4.851724e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_c152775f6f7b</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Transcripts[SEP]DNA</td>\n",
       "      <td>4.851724e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_a7799481219a</td>\n",
       "      <td>en</td>\n",
       "      <td>pt</td>\n",
       "      <td>Transcripts[SEP]DNA</td>\n",
       "      <td>4.851724e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_b68d68a3868b</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Transcripts[SEP]DNA</td>\n",
       "      <td>4.851724e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topics_ids     content_ids topic_language content_language  \\\n",
       "0  t_00069b63a70a  c_66111e868395             en               en   \n",
       "1  t_00069b63a70a  c_bd5e71a65b93             en               en   \n",
       "2  t_00069b63a70a  c_c152775f6f7b             en               en   \n",
       "3  t_00069b63a70a  c_a7799481219a             en               pt   \n",
       "4  t_00069b63a70a  c_b68d68a3868b             en               en   \n",
       "\n",
       "                  text         probs  predictions  probs_max  \n",
       "0  Transcripts[SEP]DNA  4.851724e-08            0   0.000185  \n",
       "1  Transcripts[SEP]DNA  4.851724e-08            0   0.000185  \n",
       "2  Transcripts[SEP]DNA  4.851724e-08            0   0.000185  \n",
       "3  Transcripts[SEP]DNA  4.851724e-08            0   0.000185  \n",
       "4  Transcripts[SEP]DNA  4.851724e-08            0   0.000185  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_1108dd0c7a5d c_5bc0e1e2cba0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_00068291e9a4</td>\n",
       "      <td>c_ebb7fdf10a7e c_14bf71640ecd c_ac1672cdcd2c c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_0006d41a73a8</td>\n",
       "      <td>c_b972646631cb c_0c6473c3480d c_d7a0d7eaf799 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_4054df11a74e</td>\n",
       "      <td>c_3695c5dc1df6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic_id                                        content_ids\n",
       "0  t_00004da3a1b2                      c_1108dd0c7a5d c_5bc0e1e2cba0\n",
       "1  t_00068291e9a4  c_ebb7fdf10a7e c_14bf71640ecd c_ac1672cdcd2c c...\n",
       "2  t_0006d41a73a8  c_b972646631cb c_0c6473c3480d c_d7a0d7eaf799 c...\n",
       "3  t_4054df11a74e                                     c_3695c5dc1df6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_749b9bfd3a69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic_id     content_ids\n",
       "0  t_00069b63a70a  c_749b9bfd3a69"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "topics.shape: (5, 3)\n",
      "content.shape: (154047, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33062f81e8c438a8ca53f81a6850519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f8c1ce080a487bbfa90f42daf9fb54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Training KNN model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a3248c37a54166b4f97713aba0960c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40333890e724534aaea2756238136dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics_ids</th>\n",
       "      <th>content_ids</th>\n",
       "      <th>topic_language</th>\n",
       "      <th>content_language</th>\n",
       "      <th>text</th>\n",
       "      <th>probs</th>\n",
       "      <th>predictions</th>\n",
       "      <th>probs_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_66111e868395</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Transcripts[SEP]DNA</td>\n",
       "      <td>4.851751e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_f9389c635f87</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Transcripts[SEP]RNA</td>\n",
       "      <td>4.613222e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_c152775f6f7b</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Transcripts[SEP]DNA</td>\n",
       "      <td>4.851751e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_bd5e71a65b93</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Transcripts[SEP]DNA</td>\n",
       "      <td>4.851751e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_a7799481219a</td>\n",
       "      <td>en</td>\n",
       "      <td>pt</td>\n",
       "      <td>Transcripts[SEP]DNA</td>\n",
       "      <td>4.851751e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topics_ids     content_ids topic_language content_language  \\\n",
       "0  t_00069b63a70a  c_66111e868395             en               en   \n",
       "1  t_00069b63a70a  c_f9389c635f87             en               en   \n",
       "2  t_00069b63a70a  c_c152775f6f7b             en               en   \n",
       "3  t_00069b63a70a  c_bd5e71a65b93             en               en   \n",
       "4  t_00069b63a70a  c_a7799481219a             en               pt   \n",
       "\n",
       "                  text         probs  predictions  probs_max  \n",
       "0  Transcripts[SEP]DNA  4.851751e-08            0    0.00019  \n",
       "1  Transcripts[SEP]RNA  4.613222e-08            0    0.00019  \n",
       "2  Transcripts[SEP]DNA  4.851751e-08            0    0.00019  \n",
       "3  Transcripts[SEP]DNA  4.851751e-08            0    0.00019  \n",
       "4  Transcripts[SEP]DNA  4.851751e-08            0    0.00019  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_1108dd0c7a5d c_5bc0e1e2cba0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_00068291e9a4</td>\n",
       "      <td>c_ebb7fdf10a7e c_639ea2ef9c95 c_14bf71640ecd c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_0006d41a73a8</td>\n",
       "      <td>c_b972646631cb c_0c6473c3480d c_d7a0d7eaf799 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_4054df11a74e</td>\n",
       "      <td>c_3695c5dc1df6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic_id                                        content_ids\n",
       "0  t_00004da3a1b2                      c_1108dd0c7a5d c_5bc0e1e2cba0\n",
       "1  t_00068291e9a4  c_ebb7fdf10a7e c_639ea2ef9c95 c_14bf71640ecd c...\n",
       "2  t_0006d41a73a8  c_b972646631cb c_0c6473c3480d c_d7a0d7eaf799 c...\n",
       "3  t_4054df11a74e                                     c_3695c5dc1df6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_186fc761585b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic_id     content_ids\n",
       "0  t_00069b63a70a  c_186fc761585b"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "topics.shape: (5, 3)\n",
      "content.shape: (154047, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3edf6ae6d146c890505af4413f42e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d497e01782894f3587caa06d35b11fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Training KNN model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d7cfaabb5e4012961b2129b32b9043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f8043f2b1f4df3b82a3f4dfb7efe59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics_ids</th>\n",
       "      <th>content_ids</th>\n",
       "      <th>topic_language</th>\n",
       "      <th>content_language</th>\n",
       "      <th>text</th>\n",
       "      <th>probs</th>\n",
       "      <th>predictions</th>\n",
       "      <th>probs_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_83398bf2a6b9</td>\n",
       "      <td>en</td>\n",
       "      <td>ar</td>\n",
       "      <td>Transcripts[SEP]</td>\n",
       "      <td>2.698774e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_2aeda03b182e</td>\n",
       "      <td>en</td>\n",
       "      <td>ar</td>\n",
       "      <td>Transcripts[SEP]\"\"</td>\n",
       "      <td>2.902290e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_293622bd38b5</td>\n",
       "      <td>en</td>\n",
       "      <td>ar</td>\n",
       "      <td>Transcripts[SEP] ...</td>\n",
       "      <td>1.029449e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_61628bebc483</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Transcripts[SEP]Phrasal Verbs</td>\n",
       "      <td>3.826137e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_0c4b328160dd</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>Transcripts[SEP]Direct Speech</td>\n",
       "      <td>1.800586e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topics_ids     content_ids topic_language content_language  \\\n",
       "0  t_00069b63a70a  c_83398bf2a6b9             en               ar   \n",
       "1  t_00069b63a70a  c_2aeda03b182e             en               ar   \n",
       "2  t_00069b63a70a  c_293622bd38b5             en               ar   \n",
       "3  t_00069b63a70a  c_61628bebc483             en               en   \n",
       "4  t_00069b63a70a  c_0c4b328160dd             en               en   \n",
       "\n",
       "                            text         probs  predictions  probs_max  \n",
       "0        Transcripts[SEP]  2.698774e-08            0    0.00019  \n",
       "1      Transcripts[SEP]\"\"  2.902290e-08            0    0.00019  \n",
       "2    Transcripts[SEP] ...  1.029449e-08            0    0.00019  \n",
       "3  Transcripts[SEP]Phrasal Verbs  3.826137e-08            0    0.00019  \n",
       "4  Transcripts[SEP]Direct Speech  1.800586e-07            0    0.00019  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_1108dd0c7a5d c_5bc0e1e2cba0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_00068291e9a4</td>\n",
       "      <td>c_639ea2ef9c95 c_ebb7fdf10a7e c_14bf71640ecd c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_0006d41a73a8</td>\n",
       "      <td>c_b972646631cb c_0c6473c3480d c_d7a0d7eaf799 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_4054df11a74e</td>\n",
       "      <td>c_3695c5dc1df6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic_id                                        content_ids\n",
       "0  t_00004da3a1b2                      c_1108dd0c7a5d c_5bc0e1e2cba0\n",
       "1  t_00068291e9a4  c_639ea2ef9c95 c_ebb7fdf10a7e c_14bf71640ecd c...\n",
       "2  t_0006d41a73a8  c_b972646631cb c_0c6473c3480d c_d7a0d7eaf799 c...\n",
       "3  t_4054df11a74e                                     c_3695c5dc1df6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_186fc761585b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic_id     content_ids\n",
       "0  t_00069b63a70a  c_186fc761585b"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_1108dd0c7a5d c_5bc0e1e2cba0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_00068291e9a4</td>\n",
       "      <td>c_ebb7fdf10a7e c_14bf71640ecd c_ac1672cdcd2c c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_749b9bfd3a69 c_186fc761585b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_0006d41a73a8</td>\n",
       "      <td>c_b972646631cb c_0c6473c3480d c_d7a0d7eaf799 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_4054df11a74e</td>\n",
       "      <td>c_3695c5dc1df6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic_id                                        content_ids\n",
       "0  t_00004da3a1b2                      c_1108dd0c7a5d c_5bc0e1e2cba0\n",
       "1  t_00068291e9a4  c_ebb7fdf10a7e c_14bf71640ecd c_ac1672cdcd2c c...\n",
       "2  t_00069b63a70a                      c_749b9bfd3a69 c_186fc761585b\n",
       "3  t_0006d41a73a8  c_b972646631cb c_0c6473c3480d c_d7a0d7eaf799 c...\n",
       "4  t_4054df11a74e                                     c_3695c5dc1df6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _idx, CFG in enumerate(CFG_list):\n",
    "    # Read data\n",
    "    tmp_topics, tmp_content = read_data(CFG)\n",
    "    # Run nearest neighbors\n",
    "    tmp_topics, tmp_content = get_neighbors(tmp_topics, tmp_content, CFG)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # Set id as index for content\n",
    "    tmp_content.set_index('id', inplace = True)\n",
    "    # Build training set\n",
    "    tmp_test = build_inference_set(tmp_topics, tmp_content, CFG)\n",
    "    # Process test set\n",
    "    tmp_test = preprocess_test(tmp_test)\n",
    "    # Inference\n",
    "    inference(tmp_test, CFG, _idx)\n",
    "    del tmp_topics, tmp_content, tmp_test\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "df_test = pd.concat([pd.read_csv(f'submission_{_idx + 1}.csv') for _idx in range(len(CFG_list))])\n",
    "df_test.fillna(\"\", inplace = True)\n",
    "df_test['content_ids'] = df_test['content_ids'].apply(lambda c: c.split(' '))\n",
    "df_test = df_test.explode('content_ids').groupby(['topic_id'])['content_ids'].unique().reset_index()\n",
    "df_test['content_ids'] = df_test['content_ids'].apply(lambda c: ' '.join(c))\n",
    "\n",
    "df_test.to_csv('submission.csv', index = False)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
